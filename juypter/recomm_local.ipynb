{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/lindachen/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#file = open('../Resources/' + filename, 'r', encoding=\"ISO-8859-1\")\n",
    "df = pd.read_csv(\"data/EA_CSV.csv\", engine=\"python\", encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn the first row into column\n",
    "def get_data(df):\n",
    "    #combine the description\n",
    "    df['description'] = df['Expertise'].fillna('') + \" \" + df['Title'].fillna('')\n",
    "    return df\n",
    "df = get_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    #removeNonAscii\n",
    "    s = \"\".join(i for i in s if ord(i)<128)\n",
    "\n",
    "    #return all lower cases\n",
    "    s = s.lower()\n",
    "\n",
    "    #remove stop wrods\n",
    "    s = s.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in s if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    #remove html\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    text = html_pattern.sub(r'', text)\n",
    "\n",
    "    #remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]',\" \",text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_model(file_path='glove.twitter.27B.200d.txt'):\n",
    "  \"\"\"\n",
    "  input: the access to the file which contains the pre-trained glove model\n",
    "  ouput: a trained model using glove\n",
    "  \"\"\"\n",
    "\n",
    "  embeddings_index = {}\n",
    "  f = open(file_path, encoding='utf-8')\n",
    "  for line in f:\n",
    "      values = line.split()\n",
    "      word = values[0]\n",
    "      coefs = np.asarray(values[1:], dtype='float32')\n",
    "      embeddings_index[word] = coefs\n",
    "  f.close()\n",
    "  return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove2vec(text, model):\n",
    "    \"\"\"\n",
    "    input: \n",
    "    - text: the string that you wanted to turn into vectors\n",
    "    - model: the glove pre-trained model that you wanted to use\n",
    "\n",
    "    output:\n",
    "    an average vector of your input text using the glove model\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creating a list for storing the vectors (description into vectors)\n",
    "    global word_embeddings\n",
    "    word_embeddings = []\n",
    "\n",
    "    vectors = {}\n",
    "\n",
    "    avgword2vec = None\n",
    "    count = 0\n",
    "\n",
    "    for word in text.split():\n",
    "        if word in model:\n",
    "            count += 1\n",
    "            if avgword2vec is None:\n",
    "                avgword2vec = model[word]\n",
    "            else:\n",
    "                avgword2vec = avgword2vec + model[word]\n",
    "            \n",
    "    if avgword2vec is not None:\n",
    "        avgword2vec = avgword2vec / count\n",
    "    \n",
    "    word_embeddings.append(avgword2vec)\n",
    "\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendations_glove(input, df=df, col='description'):\n",
    "    \"\"\"\n",
    "    input: \n",
    "    - input: key words relating to the article\n",
    "    - df: the dataframe that we are using\n",
    "    - col: the column name that contains description of EAs\n",
    "    - file path: the file path to the pre-trained glove model\n",
    "\n",
    "    output:\n",
    "    a list of EAs' names ranked by the most recommended / most suitable EA\n",
    "    to the least\n",
    "    \"\"\"\n",
    "\n",
    "    #get the model (it is too slow to compute the model every time)\n",
    "    model = glove_model(file_path='glove.twitter.27B.200d.txt')\n",
    "\n",
    "    # vecterizd the input\n",
    "    vector_input = glove2vec(input, model)\n",
    "\n",
    "    # vecterized all the descriptions of EAs\n",
    "    df[col] = df[col].apply(clean_text)\n",
    "    EAs_vectors = [glove2vec(EA, model) for EA in df[col]]\n",
    "\n",
    "    #drop an EA if that EA has \"None\" vector\n",
    "    df['vectors'] = EAs_vectors\n",
    "    for i, n in enumerate(df['vectors']):\n",
    "      if n[0] is None:\n",
    "        df = df.drop([i+1], axis=0)\n",
    "\n",
    "\n",
    "    # finding cosine similarity for the vectors\n",
    "    similarity = []\n",
    "    for n in df['vectors']:\n",
    "      scores = cosine_similarity(vector_input, n)[0][0]\n",
    "      similarity.append(scores)\n",
    "    \n",
    "    df['similarity'] = similarity\n",
    "\n",
    "\n",
    "    # sort and find the recommended movie\n",
    "    df = df.sort_values(by=['similarity'], ascending=False)\n",
    "    #res_df = df.iloc[:m]\n",
    "\n",
    "\n",
    "    return df['Names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recomm_engine(keywords, vec_df, model):\n",
    "    unpickled_model = pd.read_pickle(\"models/glove_model.pkl\")\n",
    "    \n",
    "    # vectorize the input\n",
    "    input_vector = glove2vec(keywords, model)\n",
    "\n",
    "    # finding cosine similarity for the vectors\n",
    "    similarity = []\n",
    "    for n in vec_df['vectors']:\n",
    "        scores = cosine_similarity(input_vector, n)[0][0]\n",
    "        similarity.append(scores)\n",
    "\n",
    "    vec_df['similarity'] = similarity\n",
    "\n",
    "    # sort and find the recommended movie\n",
    "    vec_df = vec_df.sort_values(by=['similarity'], ascending=False)\n",
    "\n",
    "    return vec_df.loc[::, :'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "26            Carlos Ortega\n23        Sergio Betancourt\n27           Julia Nikulski\n20             Rohan Joseph\n10               David Chen\n4           Gitansh Khirbat\n17              Mael Fabien\n15             Hessie Jones\n5           Abdullah Farouk\n12          Francisco Nunes\n8            Arman Didandeh\n16             Lester Leong\n19      Pier Paolo Ippolito\n2               Sophie Mann\n25        Fran√ßois St-Amant\n1            Lowri Williams\n9             Carlos Mougan\n21            John Jagtiani\n13         Sara A. Metwalli\n31               Agni Kumar\n22                 Rui Geng\n7          Anton Muehlemann\n14         Gerasimos Plegas\n32                Dean Deng\n0                Amber Teng\n3                   Jingles\n24             Sohaib Ahmad\n11    Dimitris Panagopoulos\n29          Purvanshi Mehta\n6     Andrew DeCotiis-Mauro\n30         Ashwin Hariharan\n18            Manish Sharma\n28         Marin Vlastelica\nName: Names, dtype: object\n"
     ]
    }
   ],
   "source": [
    "input = 'data collection, data ethics issues/initial assumptions'\n",
    "res = recommendations_glove(input=input, df=df, col='description')\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}