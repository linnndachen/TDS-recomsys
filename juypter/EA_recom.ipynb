{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EA_recom.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMP9uILvDQOkMAXAfu9T5bR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wchen928/TDS-recomsys/blob/master/EA_recom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9rFMUM-O60X",
        "outputId": "600929a0-8200-4a27-b6bf-9d4f07e85eca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/My Drive/TDS_Tech\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "'Copy of New authors.gsheet'\t'Progress Report'\n",
            " EA.gsheet\t\t\t'Real-time Dashboard.gdoc'\n",
            " EA_recom.ipynb\t\t\t'Recommendation System Plan.gdoc'\n",
            "\"Linda's attempt to dashboard\"\t Timeline.gsheet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIqA_kqFTqYq",
        "outputId": "81fe83e3-8119-4b86-81c9-7599bed263cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "!pip install xlrd\n",
        "!pip install psycopg2\n",
        "!pip install --upgrade gspread"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: psycopg2 in /usr/local/lib/python3.6/dist-packages (2.7.6.1)\n",
            "Requirement already up-to-date: gspread in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from gspread) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.2.1 in /usr/local/lib/python3.6/dist-packages (from gspread) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from gspread) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.12.0->gspread) (50.3.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.12.0->gspread) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.12.0->gspread) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.12.0->gspread) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.12.0->gspread) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.2.1->gspread) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.2.1->gspread) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.2.1->gspread) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.2.1->gspread) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth>=1.12.0->gspread) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvSepzO4Sd7_"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy  as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OQ4m90-VqTm"
      },
      "source": [
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "worksheet = gc.open('EA').sheet1\n",
        "\n",
        "# get_all_values gives a list of rows.\n",
        "rows = worksheet.get_all_values()\n",
        "#print(rows)\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame.from_records(rows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImO5h45sj2Du"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKu0Q7JgkMaj",
        "outputId": "b1937b4b-e12a-4ff9-9cc9-2329ccfdffe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Names</td>\n",
              "      <td>Keywords</td>\n",
              "      <td>LinkedIn</td>\n",
              "      <td>Current Title</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Lowri Williams</td>\n",
              "      <td>Python, NLP</td>\n",
              "      <td>https://www.linkedin.com/in/lowriwilliams/</td>\n",
              "      <td>Research Data Scientist at Cardiff University</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sophie Mann</td>\n",
              "      <td>cognitive science/neuroscience, healthcare, ed...</td>\n",
              "      <td>https://www.linkedin.com/in/sophie-maya-mann/</td>\n",
              "      <td>Data Manager at Tennessee Department of Education</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Jingles</td>\n",
              "      <td>in heath sector (eye)</td>\n",
              "      <td>https://www.linkedin.com/in/jingles/</td>\n",
              "      <td>PhD Student | Alibaba Group | Healthcare (Eye ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Gitansh Khirbat</td>\n",
              "      <td>NLP, Statistics, Optimizations</td>\n",
              "      <td>https://www.linkedin.com/in/gkhirbat/</td>\n",
              "      <td>Machine Learning Engineer | Applied Data Science</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0  ...                                                  3\n",
              "0            Names  ...                                      Current Title\n",
              "1   Lowri Williams  ...     Research Data Scientist at Cardiff University \n",
              "2      Sophie Mann  ...  Data Manager at Tennessee Department of Education\n",
              "3          Jingles  ...  PhD Student | Alibaba Group | Healthcare (Eye ...\n",
              "4  Gitansh Khirbat  ...   Machine Learning Engineer | Applied Data Science\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiBQ_j0ekQDM",
        "outputId": "d8a6fd24-b5d0-45a6-d771-6c6b1cf20c5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#turn the first row into column\n",
        "new_header = df.iloc[0] #grab the first row for the header\n",
        "df = df.iloc[1:] #take the data less the header row\n",
        "df.columns = new_header #set the header row as the df header\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>LinkedIn</th>\n",
              "      <th>Current Title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Lowri Williams</td>\n",
              "      <td>Python, NLP</td>\n",
              "      <td>https://www.linkedin.com/in/lowriwilliams/</td>\n",
              "      <td>Research Data Scientist at Cardiff University</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sophie Mann</td>\n",
              "      <td>cognitive science/neuroscience, healthcare, ed...</td>\n",
              "      <td>https://www.linkedin.com/in/sophie-maya-mann/</td>\n",
              "      <td>Data Manager at Tennessee Department of Education</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Jingles</td>\n",
              "      <td>in heath sector (eye)</td>\n",
              "      <td>https://www.linkedin.com/in/jingles/</td>\n",
              "      <td>PhD Student | Alibaba Group | Healthcare (Eye ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Gitansh Khirbat</td>\n",
              "      <td>NLP, Statistics, Optimizations</td>\n",
              "      <td>https://www.linkedin.com/in/gkhirbat/</td>\n",
              "      <td>Machine Learning Engineer | Applied Data Science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Abdullah Farouk</td>\n",
              "      <td>deep learning</td>\n",
              "      <td>https://www.linkedin.com/in/abdullah-farouk/</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "0            Names  ...                                      Current Title\n",
              "1   Lowri Williams  ...     Research Data Scientist at Cardiff University \n",
              "2      Sophie Mann  ...  Data Manager at Tennessee Department of Education\n",
              "3          Jingles  ...  PhD Student | Alibaba Group | Healthcare (Eye ...\n",
              "4  Gitansh Khirbat  ...   Machine Learning Engineer | Applied Data Science\n",
              "5  Abdullah Farouk  ...                                                   \n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsMBNsr5VKk_"
      },
      "source": [
        "df['description'] = df['Keywords'] + \" \" + df['Current Title']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCkSJAS8WFWd",
        "outputId": "3f778b65-d681-4a02-c1e4-4ca0d70f70f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def clean_text(s):\n",
        "    #removeNonAscii\n",
        "    s = \"\".join(i for i in s if  ord(i)<128)\n",
        "\n",
        "    #return all lower cases\n",
        "    s.lower()\n",
        "\n",
        "    #remove stop wrods\n",
        "    s = s.split()\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in s if not w in stops]\n",
        "    text = \" \".join(text)\n",
        "\n",
        "    #remove html\n",
        "    html_pattern = re.compile('<.*?>')\n",
        "    text = html_pattern.sub(r'', text)\n",
        "\n",
        "    #remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]',\" \",text)\n",
        "    return text\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbbrwb1Skohb"
      },
      "source": [
        "## Build Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xTkfH65lZIh"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evQ-tEzuk5Uc",
        "outputId": "2e4bd047-3deb-4ba6-d1ee-6ed485b5403f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# Downloading the Google pretrained Word2Vec Model\n",
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "\n",
        "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "google_word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "\n",
        "# Training our corpus with Google Pretrained Model\n",
        "\n",
        "google_model = Word2Vec(size = 300, window=5, min_count = 2, workers = -1)\n",
        "google_model.build_vocab(corpus)\n",
        "\n",
        "#model.intersect_word2vec_format('./word2vec/GoogleNews-vectors-negative300.bin', lockf=1.0, binary=True)\n",
        "\n",
        "google_model.intersect_word2vec_format(EMBEDDING_FILE, lockf=1.0, binary=True)\n",
        "\n",
        "google_model.train(corpus, total_examples=google_model.corpus_count, epochs = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-30 18:37:29--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.45.62\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.45.62|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE_qBA-NlIv9"
      },
      "source": [
        "# Generate the average word2vec for the each book description\n",
        "\n",
        "def word2vez(text):\n",
        "    \n",
        "    # Creating a list for storing the vectors (description into vectors)\n",
        "    global word_embeddings\n",
        "    word_embeddings = []\n",
        "\n",
        "    vectors = {}\n",
        "\n",
        "    avgword2vec = None\n",
        "    count = 0\n",
        "    for word in text.split():\n",
        "        if word in google_model.wv.vocab:\n",
        "            count += 1\n",
        "            if avgword2vec is None:\n",
        "                avgword2vec = google_model[word]\n",
        "            else:\n",
        "                avgword2vec = avgword2vec + google_model[word]\n",
        "            \n",
        "    if avgword2vec is not None:\n",
        "        avgword2vec = avgword2vec / count\n",
        "    \n",
        "    word_embeddings.append(avgword2vec)\n",
        "\n",
        "    return word_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo4Owzejtrp-"
      },
      "source": [
        "## Recomm System\n",
        "\n",
        "Here I still have the question of wheather using cosine_similarity or Word Mover’s Distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gOCw-H7xORx"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wR8az3Bp7fN"
      },
      "source": [
        "# Recommending the Top 5 similar books\n",
        "\n",
        "def recommendations(input, df=df, col='description', m=3):\n",
        "    \n",
        "    # vecterizd the input\n",
        "    input = clean_text(input)\n",
        "    vector_input = word2vez(input)\n",
        "\n",
        "    res = {}\n",
        "    # vecterized all the EA\n",
        "    df[col] = df[col].apply(clean_text)\n",
        "    EAs_vectors = [word2vez(EA) for EA in df[col]]\n",
        "\n",
        "    df['vectors'] = EAs_vectors\n",
        "\n",
        "    for i, n in enumerate(df['vectors']):\n",
        "      if n[0] is None:\n",
        "        df = df.drop([i+1], axis=0)\n",
        "\n",
        "\n",
        "    # finding cosine similarity for the vectors\n",
        "    similarity = []\n",
        "    for n in df['vectors']:\n",
        "      scores = cosine_similarity(vector_input, n)[0][0]\n",
        "      similarity.append(scores)\n",
        "    \n",
        "    df['similarity'] = similarity\n",
        "\n",
        "\n",
        "    #\n",
        "    df = df.sort_values(by=['similarity'], ascending=False)\n",
        "    res_df = df.loc[:m+1]\n",
        "\n",
        "\n",
        "    return res_df['Names']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "919yVZQW8twG",
        "outputId": "74f61d8c-c5c9-476e-e852-2e5c997b470d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "input = 'In this article, I’ll describe what I believe are some best practices \\n\n",
        "to start a Reinforcement Learning (RL) project. I’ll do this by illustrating \\n\n",
        "some lessons I learned when I replicated Deepmind’s performance on video \\n\n",
        "games. This was a fun side-project I worked on.'\n",
        "\n",
        "res = recommendations(input=input, df=df, col='description', m=3)\n",
        "\n",
        "print(res)\n",
        "\n",
        "#print(res[res['vectors'][0] == None])\n",
        "\n",
        "#input = clean_text(input)\n",
        "#vector_input = word2vez(input)\n",
        "\n",
        "#cosine_similarities = cosine_similarity(vector_input,EAs_vectors[1])\n",
        "\n",
        "\"\"\"\n",
        "similarity = []\n",
        "count = 0\n",
        "for n in range(0, len(EAs_vectors)-1):\n",
        "  scores = cosine_similarity(vector_input, EAs_vectors[n])[0][0]\n",
        "  similarity.append(scores)\n",
        "  count += 1\n",
        "  print(count, scores)\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19    Pier Paolo Ippolito\n",
            "31             Agni Kumar\n",
            "4         Gitansh Khirbat\n",
            "Name: Names, dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nsimilarity = []\\ncount = 0\\nfor n in range(0, len(EAs_vectors)-1):\\n  scores = cosine_similarity(vector_input, EAs_vectors[n])[0][0]\\n  similarity.append(scores)\\n  count += 1\\n  print(count, scores)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iwAeR6zB7jb"
      },
      "source": [
        "## Problem:\n",
        "\n",
        "Some words like \"R\", \"Python\" \"ML\" are not in the pre-trained Word2Vec model. Thus, there were 5 EAs have the value None."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WccyAUqCMzz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}